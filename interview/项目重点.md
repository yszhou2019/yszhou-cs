## 工作

### 异步挖掘改造

背景：原本是异步，但是消费速度不足，导致OOM
方案：1 写mq 2 单独部署消费服务，消费mq，每台实例一个协程消费mq，存储到队列中，开若干协程进行消费
替代方案：下游直接消费mq然后开始执行
替代方案劣势：消费数量受限于partition，而下游实例数量取决于业务量的繁重程度，修改partition比较麻烦
如何解决：部署消费服务，消费mq，开若干协程调用下游rpc
why：业务逻辑主要在异步过程，mq解耦之后，下游可以根据业务量扩缩容
如果下游服务还用到内存队列：这么做还可以避免更新导致的内存队列清空

### Python内存泄露排查

背景：线上服务

### byteddag

除此之外，还有什么

发现了原协程的弊端
原服务是通过协程在业务代码中，对于依赖节点调用get()来自动阻塞等待完成

正确的操作是每个task通过ctx获取依赖的所有协程，对于这些节点进行join

### 热点系列服务迁移与改造

背景：1 仓库混乱 2 分支混乱 3 启动脚本混乱 4 业务轮子冗余
how：
1 迁移到统一的新repo
2 重点是pie->euler
3 降低重复的业务轮子
4 DAG并行IO降低延迟
收益：
1 更低的延迟
2 资源的收益
3 可读性、可维护性的收益



### 为什么没通过

沟通过之后，唯一的可能性是
1 第一任mentor，工作的第一个月，（第一次业界实习，各种平台工具，以及概念，比如服务鉴权，流量mesh，老框架的兼容性 当时对工作的内容理解起来也比较吃力）后续组织架构变动，很难见面联系，自己这边的进步反馈不到mentor 虽然后续9月份之后的工作处理起来都很容易，但是9月之后架构变动->见不到->没有沟通->没法反馈自己的提升
2 答辩ppt没有体现出来对于业务的理解，和设计的能力（okr有关）
3 没有把自己的进步体现出来（10月11月工作之余完成的任务）



## OS

### 技术点

mmap，COW，有栈协程的实现



### 内核态用户态如何切换

两个文件

`trap.c`

`tramopline.S`



入口函数，调用汇编函数`trampoline`

作用：switch from user to kernel space

> 保存用户进程的CPU状态到对应proc结构的trapframe中（存储当前用户进程状态）
>
> 从进程的trapframe中恢复kernel stack pointer和kernel pgtbl
>
> 执行usertrap函数（在这个函数中**执行syscall或者intr等等**）



==每个进程被OS创建的时候，对应proc结构中都会保存kernel trapframe==





`usertrap`执行完毕后，调用`usertrapret`函数

作用：return to user space

> 保存当前kernel状态到进程的proc的trapframe中
>
> 然后调用fn`userret`，从进程proc的trapframe恢复用户态的寄存器



### 三级页表

一个文件`vm.c`



进程pid->进程的proc结构体，从结构体中得到对应的pgtbl地址

然后用va和pgtbl找到对应的PTE和pa

> va包括3个9bit的index以及12bit的页内offset
>
> 分别提取3个index，逐层访问对应的pgtbl，从而得到对应的PTE

最后PTE转成PA`#define PTE2PA(pte) (((pte) >> 10) << 12)`



### 进程调度

文件

`proc.c`

`swtch.S`



`swtch`

作用：将当前CPU状态保存到old proc的context中，从new proc的context中恢复CPU状态



每个CPU各自有一个调度器进程，yield->调用swtch，恢复调度器进程

然后调度器进程选择合适的进程，上台执行





### mmap

整体：
1 VAM struct的设计，相关接口
2 mmap unmap的实现
3 lazy_alloca

背景：
1 用户程序读取文件的流程，文件内容从disk读取到kernel space，用户程序再read，也就是kernel到user的内存拷贝
2 内存拷贝有些没必要

怎么设计的：
1 VMA，kernel空间下的全局struct，struct，类似于PTE，记录pid，va，length，对应的fd对应的FILE*，offset，perm，flags
2 VMA用来做什么，完成用户程序的va内存地址->指定文件FILE指定offset的映射
3 mmap的时候发生了什么，申请VMA，写入各项信息到kernel VMA中，返回
4 然后用户程序写入的时候，如果文件对应的kernel space没有分配，就分配pa，然后调用io读取文件的offset到pa，然后安装映射va->pa

设计之后的流程：

1 mmap，进程申请VMA，配置信息
2 读写->触发page-fault，kernel分配内存，读取file，安装va->pa的映射到进程空间
3 进程直接读写kernel space



如何取消
1 逐一归还VMA
2 如果VMA权限private，那么获取va对应的pa，如果存在PTE则释放内存，否则跳过
3 如果VMA全新shared和write，那么va->pa之后
先检查pa是否存在，不存在则跳过（说明始终没有触发page-fault）
存在，则检查是否dirty，dirty则写入disk，然后释放PTE



### lazy alloc

整体：
1 修改syscall，只对proc的size进行update，不进行实际的分配内存
2 修改page-fault触发的trap，检查寄存器原因，调用lazy_alloc
3 实现lazy_alloc，分配pa，安装va->pa的映射到pgtbl



### uthread

整体：
1 有栈协程：上下文（CPU-state，各种寄存器，最重要的是pc），stack保存执行现场
2 调度：for loop+遍历
3 yield：修改当前thread为可运行，选择下一个可运行的协程，调用汇编，保存当前的uthread线程，读取下一个uthread的现场
4 create：申请struct结构，设置func到$ra寄存器，设置stack pointer到sp寄存器





## 网盘

### 技术点

服务端：传输时的零拷贝，传输方案的设计（分块传输，多客户端加速），秒传
客户端：扫描的设计（文件meta信息+md5）



### 方案的设计



### server侧

#### 启动流程

0 server只处理连接，之后fork子进程epoll，父进程断开连接

> 为什么要多进程
> 为什么不能在server这里IO多路复用
> 传输文件会导致server没有响应
>
> （问题在于，非阻塞socket的读写，仍然是阻塞的，只是读写失败的时候直接返回而已）
>
> 既然这样，还有必要用epoll吗，没有必要，完全可以用select
>
> 之后的改进方案：
> 1 fork之后，子进程只用select处理逻辑（本质上仍然是一个client对应一个子进程）弊端：没有充分利用子进程的潜力，有些浪费资源。但是这是无法避免的，子进程非阻塞socket执行IO，IO的时候仍然是阻塞的。如果子进程去处理连接新客户端的接入，就会导致客户端无响应。只要同步IO，且关系到比较大文件IO，一定会导致客户端的无响应。
>
> 2 能不能同步IO，父进程只处理比较小的请求（不包含文件的传输），用子进程，或者协程来处理大文件的传送，即使是同步IO？比如遇到传输文件类型的请求，就用一个协程或者线程来单独进行IO，IO完毕告知主进程？
>
> 3 或者server直接多路监听，使用asio？
>
> 3 如何充分利用子进程？如何使用多路复用来避免一个client就fork一次？



1 server侧，连接数据库，启动后以守护进程方式运行。每次一个新链接之后fork子进程，子进程单独处理

2 子进程单独epoll，不断循环，读取数据，解析请求，执行命令，返回响应
发现断开连接，或者非法请求，子进程退出



#### 文件存储

文件整体存储，分块传输



#### 文件传输方案

1 md5秒传
2 多客户端对于相同文件的加速上传（分块）
3 任务队列是线程安全的，可以开多个协程来加速



#### 用户的文件组织方案

uid->vfile->pfile

保存对应实际文件的上传情况 其中，字段pfile_chunks，保存的是序列化的json，而对应的json实际上是所有chunk的上传情况（chunk序号，chunksize，是否上传完毕）



#### req和res的收发

server接收和发送req和res，用户buffer

json->转string->sendn 把buffer发送

recv req->read读取到尾零，表示读取到一个完整json->buffer转json



#### 文件io与网络io

splice的零拷贝

kernel space下文件buffer<->网络buffer



### client侧

#### 持久化

uid对应db文件（包括信息，目录，req队列，res队列）



#### 任务队列

`req`队列，保存的是待发送的请求

`res`队列，保存的是已经发送但是还没有收到回复的请求。收到回复之后，则将任务从`res`队列中删除



#### 比对扫描

本次扫描结果与之前的db文件比对 => 产生上传任务

之前的db与本次扫描结果比对 => 产生删除任务

本地与服务器目录比对 => 产生下载任务

产生的任务全部添加到`req`队列中





## 代理

### 技术点

buffer的拼接问题，IO多路复用



### proxy

LT epoll



#### 为什么要多路复用

要支持多个client同时访问



#### 怎么处理粘包问题

用户空间一个client各自一个接收buffer，发送buffer



怎么处理EPOLLIN

buffer有空间并且监听了这个socket的EPOLLIN，那么读入
然后监听对端的EPOLLOUT



怎么处理EPOLLOUT

有EPOLLOUT，并且buffer有数据
那么写入，如果仍然有，则memmove



### 客户端







## 分布式





通信的目的是什么

同步log

同步log干什么用，交给各自的状态机去按照log序列执行操作







raft的总体理解

> raft是一致性模块，保证主从机器的log是一致的（raft把log提交给了数据库，数据库按照log的index顺序执行command，并在一定时间内将数据库内容可持久化到磁盘中）





### state



log

> 结构体
>
> log 包含两个数据{command 和 term}
> 表示对应的操作和日志的term





log的分类

> 正常情况下，commitindex将log划分两部分{已经提及的，没有提交的}
>
> 对于已经commit的log，根据last applied又划分两部分{已经交给状态机执行的，没有执行的}
>
> 对于已经执行的log，又划分两部分{已经持久化到状态机的，没有持久化的}
>
> ![img](https://yszhou.oss-cn-beijing.aliyuncs.com/img/v2-9d303350c85bf23d0d2930e4c65244e1_1440w.jpg)
>
> 也就是
> 还没有提交的
>
> 已经提及了，但还没有执行的（状态机还没有执行）
>
> 已经执行了，但还没有持久化的
>
> 已经持久化的





commitindex和last applied的含义、区别

> 两者都是下标
>
> commitindex指的是**已经**被大多数节点保存的日志的最新的位置；
> lastapply index是这些**已经**被应用到状态机的日志的最新的位置。只有日志被大多数节点commit之后，commitindex才会被更新，之后才可以被apply。
>
> 
>
> 区别
>
> 首先集群更新commit index
> 然后每个实例才会各自逐步进行apply



为什么不能持久化

> （如果持久化，会出现什么问题）
>
> 根据下图（==我觉得这个图做的不对，log9-log10应该是last applied，log12到log13应该是commitindex==）
>
> ![img](https://yszhou.oss-cn-beijing.aliyuncs.com/img/v2-9d303350c85bf23d0d2930e4c65244e1_1440w.jpg)
>
> 假设现在持久化了last applied 和commitindex
>
> 几个前提
> 1 last applied到commitindex的log，一定会被apply
> 2 last applied之前的log{包括已经持久化的、没有持久化的}，不会再次apply
>
> 这就导致，重启之后已经执行但是没有持久化的log，对应的状态丢失
>
> 
>
> 不持久化，那么重启之后如何重置last applied和commitindex呢
> 将两个index全部重设为last include index（也就是已经执行并且已经持久化的index），这样子，就可以避免状态丢失
>
> 为什么 Raft 的 ApplyIndex 和 CommitIndex 不需要持久化？ - 知乎 https://www.zhihu.com/question/382888510/answer/1128147590



raft哪些状态需要持久化，哪些状态不需要持久化

> 本质上问的是「Raft instance中哪些节点不持久化会影响算法安全性」
>
> 为什么要保证算法安全性的前提下，尽可能减少需要持久化的变量？
> 由于持久化带来的开销
> 持久化成本高，开销大（阻塞io导致的停等）



raft instance的状态变量

> **log相关变量(commitindex; last applied; matchindex[]; nextindex[])**
>
> nextindex[] 和 matchindex[]
>
> 状态含义：
> nextindex[] leader侧知道的集群每个节点下次要发送的log的下标
> matchindex[] leader侧知道的集群每个节点目前和自己已经匹配的下标
>
> **这两个不需要持久化**：
> 本身就是leader所独有的属性。当他们崩溃后，集群会有其他节点选上；而其他节点选上时这两个值应当和follower相互沟通后才能准确确认；leader崩溃恢复后，由于不再是leader了，因此持久化了他们也没有任何意义。因此 nextIndex[] & matchIndex[] 无需被持久化。
>
> 
>
> commitindex和last applied
> last applied不能持久化
> 会导致状态丢失（已经被状态机执行但是没有持久化的log，重启之后不会再次被执行）
>
> 
>
> ___
>
> 
>
> currentTerm和votedFor
> 需要持久化：
> 目的，用来确保每个任期只有最多一个Leader
> 考虑场景（假设这两个状态不进行持久化）
> 节点1和节点2网络屏障，节点3刚投给节点1，然后故障重启，节点3忘记了自己的状态，又收到节点2的请求，投票给节点2。
> 导致问题，集群同一任期中出现了2个leader
>
> 
>
> votedFor
> 对于 3 台服务器：
>
> - S1 获得 S1 和 S2 的投票，并且成为任期 2 的 Leader
> - S2 重启，votedFor 变为 None
> - S3 发起了 term == 2 的投票，并获得 S2 和 S3 的选票，成为了任期 2 的 Leader
> - 此时，S1 和 S3 都可以在任期 2 同一 index 的日志记录上提交不同的值。
>
> 违反了Raft的Safety保证：State Machine Safety
>
> 
>
> log[]
> 需要持久化：
> 对于 3 台服务器：
> s1和s2网络屏障
>
> - S1 成为任期 2 的 Leader，并在自己和 S3 上追加写了 <index=1, term=2, value=X>，然后设置 commitIndex=1，最后返回已提交的值 X 给客户端
> - S3 重启，并且丢失了其日志中的记录
> - S2（具有空的日志）成为任期 3 的 Leader，因为它的空日志也至少与 S3 一样完整。S2 在自己和 S3 上追加写 <index=1, term=3, value=Y>，并设置commitIndex=1，然后返回已提交的值 Y 给客户端
>
> 违反了Raft的Safety保证：leader completeness
>
> 
>
> 







leader收到客户端命令

> 收到log之后，首先保存到自己的logs中，然后再并行rpc等过半follower收到log，leader再更新commitindex，并告知客户端









#### 状态转换



#### 三个rpc的细节









leader 选举

reset timer

> 如果仔细阅读过 Students' Guide to Raft，其实里面很清楚写着，选举超时时间只能在下面三种情况下重置：
>
> - 收到现任 Leader 的心跳请求，如果 AppendEntries 请求参数的任期是过期的(`args.Term < currentTerm`)，不能重置；
> - candidate开始了一次选举；
> - **follower投票给了别的节点**（没投的话也不能重置）；



投票原则

>  If votedFor is null or candidateId, and candidate’s log is at least as up-to-date as receiver’s log, grant vote
>
> 正确理解应该是：节点只能向满足下面条件之一的候选人投出赞成票：
> \1. last log term 大者优先：候选人最后一条Log条目的任期号大于本地最后一条Log条目的任期号；
> \2. 平term，长度优先： 或者，候选人最后一条Log条目的任期号等于本地最后一条Log条目的任期号，且候选人的Log记录长度大于等于本地Log记录的长度





日志复制



两个/三个rpc的细节



用于触发超出选举的timer，在voteRPC和appendRPC期间，是处于停止状态，还是正常工作状态？



候选人主动进行voteRPC，接收到的res中自己的term更小，自己需要转化到candidate状态还是follower状态？



leader进行appendRPC，接收到的res自己的term更小，自己需要回退到follower状态？



无论是follower还是leader还是candidate，被动接收到别人的RPC（可能是vote可能是append）
如果自己的term更小，会发生什么？
如果自己的term更大，会发生什么？



收到voteRPC或者appendRPC的时候，会不会重置timer呢









三个状态的转换



