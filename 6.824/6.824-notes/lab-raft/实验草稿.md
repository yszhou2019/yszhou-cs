

>
>
>AppendEntries请求只有leader发送
>
>ReqVote请求只有candidate发送
>
>follower和candidate



## state

**所有rf都有的持久化数据**

1 currentTerm 本rf已知的最新term
2 votedFor 当前term接收到的拉票candidate的id，如果当前rf没有进行投票，那么为nil
3 log[] 日志

**所有rf都有的内存的数据（宕机则丢）**

1 commitIndex 当前已经提交的log中最新的log的index
2 lastApplied ???应用到状态机中的log的最高index

**leader的状态（选举之后会被初始化）**

1 nextIndex[] 发送给对应rf的log的下一条log的索引
2 matchIndex[] ???



## 追加新数据 rpc

leader 用于log同步，同步到fellow，用于向fellow和candidate发送心跳包

args
1 term 自己的term
2 leaderID 自己的id（用途，客户端可能会把req发送到fellow，fellow接收到知道leaderID，从而将req进行重定向
3 prevLogIdx 新log前一条log的idx
4 prevLogTerm 新log前一条log的term
5 entries[] 用作心跳包，则为空；否则保存的是需要fellow同步的数据
6 leaderCommit leader最新的log的idx

replys
1 term 当前知道的最新term（可能选举出来的新leader，从而本leader需要回退成fellow）
2 success

rpc对象如何处理？
1 leader的term<接收者的currentterm，返回false（这种情况发生于比如leader网络故障，心跳包没有发出去，在此期间fellow和candidate进行了新的选举）从而当前leader的term小于新leader的term
2 如果receiver 的log的idx和term匹配了args中的log的idx和term，匹配上了继续进行，否则返回false
3 如果log冲突（索引相同，term不同），那么删除掉idx以及之后的log
4 将args中的entries[]进行append
5 leader已知的...???



## RequestVote RPC

拉票rpc



## rf需要遵守的规则

**所有rf都需要遵守的**





## Raft核心概念

raft是一致性算法，为了维护多机状态机的一致性，首先需要保证日志在多机上的一致性。也就是说，raft是在多机之间复制日志并保证日志的一致性的算法。（复制日志的算法）

谁来保证一致性？leader的责任就是保证log复制到其他机器上之后的一致性





## 2A TODO



投票RPC相关要求

1 完善 requestVote RPC的args和replys
2 完善 server需要遵守的与选举相关的规则
3 raft结构体中添加选举状态
4 额外定义一个结构体，存储每条log的信息
5 完善RequestVote的Args结构和Reply结构，修改Make()，创建后台goroutine，周期性触发RequestVote RPC（长时间没有听到leader的心跳包）按照这种方式，这个rf实例可以通过RPC了解到谁是leader，是否有leader，或者自己成为leader。
完善投票RPC handler：其他rf实例（follower）通过这个函数进行投票



周期性触发拉票RPC，周期是否应该相同？拉票RPC的等待时间是随机生成的。周期性触发RPC，也就是周期性没有听到心跳包。那么，该等于心跳包的周期吗？200ms的心跳包周期，等待心跳包的时间应该设置成多少???

对于非leader的rf，make()之后，创建goroutine，300ms没有收到心跳包，就触发拉票RPC（向其余所有的rf实例，包括leader，发送拉票RPC）拉票RPC的等待时间是随机生成的，自己成为candidate
接收到RPC回复之后，如果有term更大的leader（或者term相同，其他人成为leader），那么自己就更新term和现任leader的坐标，然后自己回退到follower
如果自己赢得了票选，自己成为leader，那么200ms周期性执行心跳RPC
如果都没有人赢得票选，那么重新开始新一轮的拉票RPC

candidate期间自己是否应当对其他拉票RPC进行回复？怎么回复？
暂时假定不会给其他人投票，也不会更新其他人的term

follower对于怎么对多个拉票RPC进行回复？
（逻辑参照figure 2）

leader怎么回复？（比如old leader重新上线）
这个涉及的是old leader发送心跳包，其他follower如何对心跳包做出回复







心跳RPC相关要求

6 完善两个AppendEntries RPC结构，Args Reply结构，领导者周期性的发送args
7 完善AppendEntries RPC handler，要求：接收到心跳包之后，重置计时器（计时器是用来触发RequestVote RPC的）
8 对于触发RequestVote RPC函数的raft实例（也就是candidate），等待投票的timeout需要随机设置（避免瓜分投票导致没有选举出leader）



心跳RPC返回结构体的定义

1 currentTerm，用于leader更新（old leader掉线重连）
2 success，心跳成功（则leader仍然是leader；心跳失败，leader回退到follower状态）

leader单独开一个goroutine，周期性对其余所有raft发送心跳RPC
（正常情况下）其他raft如何回复？接收到心跳之后，重置拉票RPC倒计时，并进行回复
（old leader重新连接成功）其他raft如何回复？



心跳周期的时间要求，投票RPC的等待时间要求

9 要求：leader发送心跳包RPC的频率不高于每秒10次（周期>=100ms）
10 要求：old leader宕机之后，5秒之内选举出新leader。（这就要求心跳包RPC 周期和拉票RPC的等待时间比较短）
11 要求：paper中的选举等待时间是150~300ms，只有当心跳包周期通常是150ms的时候才意义。也就是说，投票RPC的等待时间需要大于心跳包周期）
12 go建议：周期性执行或者延迟一段时间后执行，建议的做法：创建goroutine，内部进行for循环，循环内部调用`time.Sleep()`
13 go建议：lock的建议，raft-structure的建议
14 完善`GetState()`
15 测试程序调用`rf.Kill()`来永久关闭一个rf实例。可以通过rf.killed()函数可以检测。（放到loop循环中避免已经关闭的rf打印信息）
16 程序输出重定向`go test -run 2A > out`。`util.go`中的DPrintf可以用用，debug起来比较方便
17 RPC的结构体首字母大写，结构体内部的结构体也是首字母大写
18 线程安全问题，可以用`go test -race`进行检测

心跳包RPC周期200ms，投票RPC等待时间300ms





## 一开始尝试的状态机思路



server状态机



Important! 6.248的RPC调用是同步调用

```bash
follower:
	set timer=random-set(250ms-300ms)
	时间到了之后，转入candidate状态
	如果接收到AppendEntriesRPC或者RequestVoteRPC，那么重新set timer

candidate:
	term++
	set timer=random-set(250ms-300ms)
	对其余所有的server进行RequestVoteRPC
	
	timer到达之后进行判断？还是说阻塞判断？还是说RPC收到reply之后每一个handler都进行一次判断？
	判断逻辑：
	if 自己赢得了majority votes，那么自己->leader
	else if 别人赢得了leader，那么自己->follower
	else 没有人赢得leader，那么自己->candidate

leader:
	goroutine-1:
	# 接收client端的指令，并将log同步到其他raft-server中
		接收到client的logs之后
		append(rf.log)
		reply client
		call AppendEntries(logs[])RPC
		大多数server都comiit之后，leader进行apply？
	goroutine-2:
	# 周期性心跳
		for{
			set timer=200ms
			200ms触发之后，对其余所有的server
				call AppendEntries(空log[])RPC
		}set timer=200ms
		
```



## 同步RPC出现的问题

思路没啥问题，关键是lab-2A第二个测试，宕机之后的连接测试

出现问题：同步RPC导致的超时问题

leader宕机之后有follower转化成候选者，候选者本身已经赢得2票，应当转化成leader，但是由于 req Vote RPC 对象还有old leader，因此导致RPC超时，从而另外一个follower等不到信息，转化成candidate。同样的，这个candidate可能会赢得2票，但是仍然会由于old leader 同步RPC导致超时。

从而，始终会保持vote split，不会有leader选举产生。

## 解决

采用for loop + go，开启多线程，多线程同时进行RPC，并且对RPC超时进行限制，超出一定的等待时间之后就不再等待，
有两个步骤都需要完成才能避免同步RPC超时

1 再开启一个线程单独运行RPC
2 主线程中用channel等待子线程的RPC结果，用select进行等待

```go

	for i := range rf.peers {
		if i == rf.me {
			continue
		}

		wg.Add(1)
		go func(server int) {
			defer wg.Done()
			reply := RequestVoteReply{}
			resCh := make(chan int)
			go func() {
				rf.sendRequestVote(server, &args, &reply)
				resCh <- 1
			}()
      // 避免RPC严重超时的解决方法在这里，
			select {
			case <-time.After(RPC_CALL_TIMEOUT):
				return
			case <-resCh:
				replyCh <- reply
			}
		}(i)
	}
```











>
>
>其他server如何处理拉票
>
>首先确保有资格，就是必须满足term>=自己的term
>
>然后，如果自己没有投票votedFor=-1，那么vatedFor=candiadteID，并且给予投票
>
>（投过票之后，不能给同一届的再投票）





比如candidate，循环的时候，如果状态发生改变，那么自己必须退出循环



>
>
>处理req vote
>
>如果自己是follower，那么考虑考虑投票
>
>如果自己是candidate，绝对会false，不过，可能会更新一下term





>
>
>选票过程中，采用同步RPC，掉线leader导致超时，本身这个候选人已经选举成功。
>
>但是超时，触发了其他候选人开始选举。

发生什么情况？

有两个follower 0 1 

0已经赢得了1的vote ，1的选举倒计时已经reset了

但是和掉线的leader进行RPC的时候，1的倒计时已经到了，导致1开始进行选举



关键一点是，和掉线leader进行RPC的时候，是阻塞的，RPC严重超时，应当忽视掉





>
>
>之前最大的问题是，leader已经选出出来，但是同步RPC导致leader选举的时候有一个阻塞相当长的时间。导致其他follower超时开始选举。
>
>这里的解决方案是，leader选举的时候多线程选举，选举出来之后开始进行心跳。（心跳周期100ms，小于RPC）选举成功之后应当立即心跳一次

感觉好像还是有可能出现这种情况啊

比如一个候选者选举成功，term=3，还没来得及发送心跳RPC的时候，另外一个候选者自己没有选举成功，刚好要开始下一轮的投票。（这种情况实际上会发生，不过刚好这个新的候选者term=4会选举成功）



## lab 2b

run test -2b的时候，如果real time超过1min，或者user cpu时间超过5s

那么代码可能有问题

1 RPC等待时间太长
2 循环中需要结合sleep或者条件变量或者channel



要求
1先过测试TestBasicAgree2B() ，从Start()函数开始实现， 
2 
3 
4
5
6 代码可能需要重构